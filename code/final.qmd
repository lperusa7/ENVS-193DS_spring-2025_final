---
title: "Final"
author: "Lukas Lescano"
date: "06-09-2025"
format:
  html:
    toc: true
    toc-depth: 3
    toc-location: left
    toc-floating: true
execute:
  message: false
  warning: false 
---

The link to my GitHub repository is
[here](https://github.com/lperusa7/ENVS-193DS_spring-2025_final)

## Setup

```{r, packages and data}
# Reading in pakages
library(tidyverse)
library(janitor)
library(here)
library(flextable)
library(DHARMa)
library(readxl)
library(MuMIn)
library(ggplot2)
library(ggeffects)

# Loading in data
sst <- read_csv(
  here("data", "sst.csv") # file path
)

nestbox_data <- read_csv(
  here("data", "occdist.csv") # file path
)
```

# Problem 1. Research writing

## a. Transparent statistical methods

In part 1, my co-worker used a **correlation test**, specifically a **Pearson's correlation**, to test the linear relationship between **distance from headwater** and **annual total nitrogen load**. We know this because of the phrasing "there is no correlation" which indicates that they tested the correlation between these two continuous variables, which knowing the variables are continuous tells us to use a Pearson's r correlation.
In part 2, my co-worker used a Analysis of Variance (ANOVA) test to be able to asses whether there are any statistically significant differences in the **average nitrogen load** across **multiple sources** (urban land, atmospheric deposition, fertilizer, wastewater treatment, and grasslands). This is evident because the nature of an ANOVA is that the test compares means across more than two groups, which is suggested by the wording "difference in average nitrogen load between sources". 

## b. More information needed

To better interpret the results of the ANOVA in part 2 we should add two additional pieces of information for context. The first is including the **effect size**, by calculating the Eta squared. The addition of an effect size to the context essentially **quantifies the proportion of variance in nitrogen load** between the different sources. Ultimately, this helps us determine whether the effect is **small, medium, or large**, which adds significance to the results. For example, if we got an Eta squared value of 0.20, this would indicate that **20% of the variability in the response can be explained by the source**. Additionally, we should also include a **post-hoc test**, specifically a **Tukey's HSD**, because the ANOVA only tells us that **at least one group differs**, but not specifically which group(s) are significantly different. A Tukey's HSD compares all possible pairs of group means (e.g. **urban land vs. fertilizer, fertilizer vs. grasslands**). This helps clarify which specific sources differ in nitrogen load, which is essential for the interpretation of the results. For example, the test may reveal that the fertilizer group and wastewater treatment group nitrogen loads are significantly higher than those from the atmospheric deposition and grasslands.

## c. Suggestions for rewriting

Part 1:
We found that there is a **small/moderate/strong** association between **distance from headwater (km)** and **annual total nitrogen load (kg year^-1)** (Pearson's r = correlation coefficient, p = 0.03, $\alpha = significance level)

Part 2:
We found a **small/medium/large** difference (Eta squared = effect size value) between sources (urban land, atmospheric deposition, fertilizer, wastewater treatment, and grasslands) in average nitrogen load (one-way ANOVA, F(df, df) = test statistic, p = 0.02, $\alpha = significance level)
On average, average nitrogen loads in **Group A** tended to be **higher/lower** than in **Group B** (how much higher or lower from Tukey's HSD, 95% CI [range] kg year^-1), **Group C** (how much higher or lower from Tukey's HSD, 95% CI [range] kg year^-1), **Group D** (how much higher or lower from Tukey's HSD, 95% CI [range] kg year^-1), and **Group E** (how much higher or lower from Tukey's HSD, 95% CI [range] kg year^-1).

# Problem 2. Data visualization

## a. Cleaning and summarizing

```{r, cleaning-data-sst}
# Creating a clean data frame
sst_clean <- sst |> # starting with sst data frame
  clean_names() |> # cleaning column names
  rename(mean_monthly_sst = temp) |> 
  mutate(
    date = as.Date(date), # ensures date column is date type
    month = format(date, "%b"), # extract month (abbreviated)
    year = format(date, "%Y")) |>  # extract year
  filter(year %in% c("2018", "2019", "2020", "2021", "2022", "2023") # filtering only years 2018-2023
  ) |> 
  mutate(
    year = factor(year), # convert year to factor
    month = factor(month, levels = c("Jan", # ordering months correctly from January to Decemeber
                   "Feb",
                   "Mar",
                   "Apr",
                   "May",
                   "Jun",
                   "Jul",
                   "Aug",
                   "Sep",
                   "Oct",
                   "Nov",
                   "Dec"),
                   ordered = TRUE) # convert month to ordered factor
  ) |> 
  group_by(year, month) |> 
  summarise(
    mean_monthly_sst = mean(mean_monthly_sst, na.rm = TRUE), # calculate the average SST for each month-year pair
    .groups = "drop") |> # drop grouping after summarizing
  select(year, month, mean_monthly_sst) # selecting columns
```

```{r, displaying-sample-and-structure}
# Display 5 random rows from the cleaned data
sst_clean |> 
  slice_sample(n = 5)

# Display structure of the cleaned data
str(sst_clean)
```

## b. Visualize the data

```{r, storing-colors-and-theme}
# Storing colors to use for each year
yr2018_col <- "#9ecae1"
yr2019_col <- "#4292c6"
yr2020_col <- "#2171b5"
yr2021_col <- "#04408d"
yr2022_col <- "#10355b"
yr2023_col <- "#0a233c"

# Apply consistent clean theme
theme_set(theme_bw())
```


```{r, recreating-visualization}
ggplot(data = sst_clean,
       aes(x = month,
           y = mean_monthly_sst,
           group = year,
           color = year)) +
  geom_line() +
  geom_point() +
  scale_color_manual(values = c(
    "2018" = yr2018_col,
    "2019" = yr2019_col,
    "2020" = yr2020_col,
    "2021" = yr2021_col,
    "2022" = yr2022_col,
    "2023" = yr2023_col
  )) +
  labs(
    x = "Month",
    y = "Mean monthly sea surface temperature (°C)",
    color = "Year"
  ) +
  theme(
    legend.position = c(0.10, 0.70), # putting legend in the top-left inside panel
    #legend.justification = c("left", "top"),
    legend.background = element_rect(fill = "white",
                                     color = NA), # makes legend readble
    panel.grid = element_blank(), # removing gridlines
    legend.title = element_text(color = "black"),
    legend.text = element_text(color = "black")
  )
```


# Problem 3. Data analysis

```{r, cleaning-data-nestbox}
nestbox_data_clean <- nestbox_data |> 
  clean_names() |> 
  mutate(
    season = factor(season)
  )
```


## a. Response variable

The response variable is **Swift Parrot box occupancy**, which is displayed in the data set as a binary variable **"sp"**. A value of 1 in the "sp" column, represents that the nest box was occupied by a Swift Parrot, while a 0 would indicate that the nest box was not occupied by a Swift Parrot, in turn meaning that it is occupied by a Common Starling or a Tree Martin, or it was empty.

## b. Purpose of study

The Swift Parrot is the **target species** of the study and is **critically endangered**, while the Common Starlings, who were **introduced and abundant** at the study area, and Tree Martins, who are **native**, are competitors that may occupy the nest boxes instead. The Swift Parrot is the **focus of restoration efforts** and their nesting success is vital to the project's goals.

## c. Difference in “seasons”

The two seasons compared in the study are 2016, which was the year the nest boxes were newly installed, and 2019, which was three years after the nest boxes were installed. Due to the three year gap in "seasons" it allows the author to discover the nest box use changes over time, specifically viewing whether the competitors increasingly occupy the boxes intended for Swift Parrots or not. 

## d. Table of models

4 models total:

| Model number | Season |  Distance to forest edge | Model Description            | 
|:------------:|:------:|:------------------------:|------------------------------|  
| 0            |        |                          | no predictors (null model)   |
| 1            |    X   |             X            | all predictors (full model)  | 
| 2            |        |             X            | season only                  |   
| 3            |        |                          | distance to forest edge only |    

## e. Run the models

```{r, model-fitting}
#| echo: true
results = 'hide'
# model 0: null model
model0 <- glm(
  sp ~ 1, # no predictors
  data = nestbox_data_clean,
  family = binomial
)

# model 1: all predictors
model1 <- glm(
  sp ~ season * edge_distance, # full model with interaction
  data = nestbox_data_clean,
  family = binomial
)

# model 2: season only
model2 <- glm(
  sp ~ season,
  data = nestbox_data_clean,
  family = binomial
)

# model 3: distance to forest edge only
model3 <- glm(
  sp ~ edge_distance,
  data = nestbox_data_clean,
  family = binomial
)
```

## f. Check the diagnostics

```{r, model-diagnostics}
par(mfrow = c(2,2)) # creates a 2x2 grid to display diagnostic plots
plot(model1) # redisuals look ok for best model
plot(model2)
plot(model3)

# Model 1 looks the best
```

## g. Select the best model

```{r, model-selection}
AICc(model1,
     model2,
     model3,
     model0) |> 
  # arranging output in descending order of AIC
  arrange(AICc)
```

The best model, as determined by Akaike’s Information Criterion (AIC), was the model that predicts **Swift Parrot nest box occupancy** includes **season and distance to forest edge as predictors, along with their interaction**. This model had the lowest AIC value, indicating that it best balances model simplicity and fit among the considered models.

## h. Visualize the model predictions

```{r, model-predictions}
model1_predictions <- ggpredict(
  model1,
  terms = c("edge_distance [all]", "season") # predictors
) |> 
  rename(distance_to_edge = x,
         season = group)

view(model1_predictions)
```

```{r, model-visualization}
ggplot(data = nestbox_data_clean,
       aes(x = edge_distance, # x-axis
           y = sp, # y-axis
           color = season)) + # color points by season
  
  # Underlying data: jitter to show the binary 0/1 occupancy
  geom_jitter(width = 0, # no horizontal jitter
              height = 0.05, # slight vertical jitter to separate overlapping points
              alpha = 0.4) + # make points semi-transparent
  
  # Model predictions layer with a 95% CI
  geom_line(data = model1_predictions, # use model prediction data
            aes(x = distance_to_edge, # x from prediction output
                y = predicted, # predicted occupancy probability
                color = season), # color lines by season
            size = 1.2) + # set line thickness
  geom_ribbon(data = model1_predictions, 
              aes(x = distance_to_edge,
                  y = predicted,
                  ymin = conf.low, # lower CI bound
                  ymax = conf.high, # upper CI bound
                  fill = season), # full ribbons by season
              alpha = 0.2, # semi-transparent fill
              color = NA) + # no border line around the ribbon
  
  # Custom colors (non-default)
  scale_color_manual(values = c("2016" = "#D55E00", "2019" = "#0072B2")) +
  scale_fill_manual(values = c("2016" = "#D55E00", "2019" = "#0072B2")) +

  # Axis labels
  labs(
    x = "Distance to forest edge (m)", # full x-axis label
    y = "Probability of Swift Parrot occupancy", # full y-axis label
    color = "Season", # legend title for line color
    fill = "Season" # legend title for ribbon fill
  ) +

  # Removing gridlines and use clean theme
  theme_bw() +
  theme(
    panel.grid.major = element_blank(), # removing major gridlines
    panel.grid.minor = element_blank(), # removing minor gridlines
    legend.position = c(0.90, 0.0) # place legend inside top right
  )
```

## i. Write a caption for your figure.



## j. Calculate model predictions

## k. Interpret your results


# Problem 4. Affective and exploratory visualizations

## a. Comparing visualizations

## b. Sharing your affective visualization